#!/usr/bin/env python3
"""
æ¨¡å‹åˆ‡æ¢å·¥å…·
ç”¨äºåœ¨ä¸åŒè®­ç»ƒçš„æ¨¡å‹ä¹‹é—´å¿«é€Ÿåˆ‡æ¢
"""

import argparse
import os
import sys
from pathlib import Path

import yaml

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = Path(__file__).parent / "config_models.yaml"
APP_PY = Path(__file__).parent / "app.py"
API_SERVER_PY = Path(__file__).parent / "api_server.py"


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def save_config(config):
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)


def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    config = load_config()
    models = config.get('models', {})
    current = config.get('current_model', '')

    print("=" * 60)
    print("å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š")
    print("=" * 60)

    for model_id, model_config in models.items():
        is_current = model_id == current
        marker = "ã€å½“å‰ã€‘" if is_current else ""
        print(f"\n{model_id} {marker}")
        print(f"  åç§°: {model_config['name']}")
        print(f"  åŸºç¡€æ¨¡å‹: {model_config['model_name_or_path']}")
        print(f"  LoRA æƒé‡: {model_config.get('adapter_name_or_path', 'æ— ')}")
        print(f"  æ¨¡æ¿: {model_config['template']}")
        print(f"  å¾®è°ƒç±»å‹: {model_config['finetuning_type']}")
        print(f"  æè¿°: {model_config['description']}")

    print("\n" + "=" * 60)
    print(f"å½“å‰ä½¿ç”¨: {current}")
    print("=" * 60)


def switch_model(model_id):
    """åˆ‡æ¢æ¨¡å‹"""
    config = load_config()
    models = config.get('models', {})

    if model_id not in models:
        print(f"âŒ é”™è¯¯: æ¨¡å‹ '{model_id}' ä¸å­˜åœ¨")
        print(f"\nå¯ç”¨æ¨¡å‹: {', '.join(models.keys())}")
        sys.exit(1)

    # æ›´æ–°å½“å‰æ¨¡å‹
    config['current_model'] = model_id
    save_config(config)

    model_config = models[model_id]

    print("=" * 60)
    print(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_id}")
    print("=" * 60)
    print(f"  æ¨¡å‹åç§°: {model_config['name']}")
    print(f"  åŸºç¡€æ¨¡å‹: {model_config['model_name_or_path']}")
    print(f"  LoRA æƒé‡: {model_config.get('adapter_name_or_path', 'æ— ')}")
    print(f"  æè¿°: {model_config['description']}")
    print("=" * 60)
    print("\nğŸ“ é…ç½®å·²æ›´æ–°ï¼")
    print("è¯·é‡å¯åº”ç”¨ä»¥ä½¿ç”¨æ–°æ¨¡å‹ï¼š")
    print("  ./start.sh gradio   # Gradio ç•Œé¢")
    print("  ./start.sh api      # API æœåŠ¡")


def add_model(model_id, name, base_model, adapter_path, template="Qwen", finetuning_type="lora", description=""):
    """æ·»åŠ æ–°æ¨¡å‹é…ç½®"""
    config = load_config()

    if 'models' not in config:
        config['models'] = {}

    if model_id in config['models']:
        print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ '{model_id}' å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")

    config['models'][model_id] = {
        'name': name,
        'model_name_or_path': base_model,
        'adapter_name_or_path': adapter_path,
        'template': template,
        'finetuning_type': finetuning_type,
        'description': description
    }

    save_config(config)

    print(f"âœ… æ¨¡å‹ '{model_id}' å·²æ·»åŠ ")


def compare_models(model1_id, model2_id):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
    config = load_config()
    models = config.get('models', {})

    for mid in [model1_id, model2_id]:
        if mid not in models:
            print(f"âŒ é”™è¯¯: æ¨¡å‹ '{mid}' ä¸å­˜åœ¨")
            sys.exit(1)

    m1 = models[model1_id]
    m2 = models[model2_id]

    print("=" * 80)
    print(f"æ¨¡å‹å¯¹æ¯”: {model1_id} vs {model2_id}")
    print("=" * 80)

    print(f"\n{model1_id.upper()}:")
    print(f"  åç§°: {m1['name']}")
    print(f"  åŸºç¡€æ¨¡å‹: {m1['model_name_or_path']}")
    print(f"  LoRA æƒé‡: {m1.get('adapter_name_or_path', 'æ— ')}")
    print(f"  æè¿°: {m1['description']}")

    print(f"\n{model2_id.upper()}:")
    print(f"  åç§°: {m2['name']}")
    print(f"  åŸºç¡€æ¨¡å‹: {m2['model_name_or_path']}")
    print(f"  LoRA æƒé‡: {m2.get('adapter_name_or_path', 'æ— ')}")
    print(f"  æè¿°: {m2['description']}")

    print("\n" + "=" * 80)
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("=" * 80)

    # ç®€å•çš„å¯¹æ¯”é€»è¾‘
    if "7B" in m1['name'] and "1.5B" in m2['name']:
        print(f"  â€¢ {model1_id}: æ€§èƒ½æ›´å¼ºï¼Œå›ç­”æ›´å‡†ç¡®ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢")
        print(f"  â€¢ {model2_id}: é€Ÿåº¦æ›´å¿«ï¼Œå“åº”æ›´åŠæ—¶ï¼Œä½†å¯èƒ½å‡†ç¡®åº¦ç¨ä½")
    elif "1.5B" in m1['name'] and "7B" in m2['name']:
        print(f"  â€¢ {model1_id}: é€Ÿåº¦æ›´å¿«ï¼Œå“åº”æ›´åŠæ—¶ï¼Œä½†å¯èƒ½å‡†ç¡®åº¦ç¨ä½")
        print(f"  â€¢ {model2_id}: æ€§èƒ½æ›´å¼ºï¼Œå›ç­”æ›´å‡†ç¡®ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢")
    else:
        print(f"  â€¢ {model1_id}: {m1['description']}")
        print(f"  â€¢ {model2_id}: {m2['description']}")

    print(f"\nğŸ§ª å»ºè®®: å¯ä»¥åœ¨ç›¸åŒé—®é¢˜ä¸‹å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”è´¨é‡")


def get_current_model_config():
    """è·å–å½“å‰æ¨¡å‹é…ç½®ï¼ˆç”¨äº app.py å’Œ api_server.pyï¼‰"""
    config = load_config()
    current_id = config.get('current_model', 'qwen-7b')
    models = config.get('models', {})

    if current_id not in models:
        print(f"âš ï¸  è­¦å‘Š: å½“å‰æ¨¡å‹ '{current_id}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {}

    return models[current_id]


def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹åˆ‡æ¢å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')

    # list å‘½ä»¤
    subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹')

    # switch å‘½ä»¤
    switch_parser = subparsers.add_parser('switch', help='åˆ‡æ¢æ¨¡å‹')
    switch_parser.add_argument('model_id', help='æ¨¡å‹ ID')

    # compare å‘½ä»¤
    compare_parser = subparsers.add_parser('compare', help='å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹')
    compare_parser.add_argument('model1', help='ç¬¬ä¸€ä¸ªæ¨¡å‹ ID')
    compare_parser.add_argument('model2', help='ç¬¬äºŒä¸ªæ¨¡å‹ ID')

    # add å‘½ä»¤
    add_parser = subparsers.add_parser('add', help='æ·»åŠ æ–°æ¨¡å‹')
    add_parser.add_argument('--id', required=True, help='æ¨¡å‹ ID')
    add_parser.add_argument('--name', required=True, help='æ¨¡å‹åç§°')
    add_parser.add_argument('--base', required=True, help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    add_parser.add_argument('--adapter', help='LoRA æƒé‡è·¯å¾„')
    add_parser.add_argument('--template', default='Qwen', help='æ¨¡æ¿ç±»å‹')
    add_parser.add_argument('--finetuning', default='lora', help='å¾®è°ƒç±»å‹')
    add_parser.add_argument('--desc', default='', help='æ¨¡å‹æè¿°')

    args = parser.parse_args()

    if args.command == 'list':
        list_models()
    elif args.command == 'switch':
        switch_model(args.model_id)
    elif args.command == 'compare':
        compare_models(args.model1, args.model2)
    elif args.command == 'add':
        add_model(
            args.id, args.name, args.base, args.adapter,
            args.template, args.finetuning, args.desc
        )
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
