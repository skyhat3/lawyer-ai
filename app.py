#!/usr/bin/env python3
"""
å¾‹å¸ˆ AI å¤§æ¨¡å‹å‰ç«¯åº”ç”¨
åŸºäº Gradio å’Œ FastAPI çš„éƒ¨ç½²æ–¹æ¡ˆ
"""

import os
import re
import yaml
import gradio as gr
from typing import List, Tuple
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

from llamafactory.chat import ChatModel


# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = Path(__file__).parent / "config_models.yaml"


def load_model_config():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹é…ç½®"""
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            current_id = config.get('current_model', 'qwen-7b')
            models = config.get('models', {})
            if current_id in models:
                return models[current_id]
            else:
                print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ '{current_id}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        return {}


# æ³•è§„å…³é”®è¯æå–å’Œè¶…é“¾æ¥ç”Ÿæˆé…ç½®
LAW_PATTERNS = [
    # æ³•æ¡æ ¼å¼ï¼šç¬¬Xæ¡ã€ç¬¬Xæ¬¾ç­‰
    r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶]+æ¡',
    r'ç¬¬[0-9]+æ¡',
    r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶]+æ¬¾',
    r'ç¬¬[0-9]+æ¬¾',
    r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶]+é¡¹',
    r'ç¬¬[0-9]+é¡¹',
    # å¸¸è§æ³•å¾‹æ³•è§„åç§°
    r'ã€Š[^ã€‹]+æ³•ã€‹',
    r'ã€Š[^ã€‹]+æ¡ä¾‹ã€‹',
    r'ã€Š[^ã€‹]+è§„å®šã€‹',
    r'ã€Š[^ã€‹]+åŠæ³•ã€‹',
    r'ã€Š[^ã€‹]+ç»†åˆ™ã€‹',
    r'ã€Š[^ã€‹]+è§£é‡Šã€‹',
]


class LawyerChatApp:
    def __init__(self):
        """åˆå§‹åŒ–å¾‹å¸ˆ AI èŠå¤©åº”ç”¨"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

        # ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹é…ç½®
        model_config = load_model_config()

        if model_config:
            print(f"ä½¿ç”¨æ¨¡å‹: {model_config['name']}")
            args = {
                "model_name_or_path": model_config['model_name_or_path'],
                "adapter_name_or_path": model_config['adapter_name_or_path'],
                "template": model_config['template'],
                "finetuning_type": model_config['finetuning_type'],
            }
            print(f"  - åŸºç¡€æ¨¡å‹: {model_config['model_name_or_path']}")
            print(f"  - LoRA æƒé‡: {model_config['adapter_name_or_path']}")
        else:
            print("ä½¿ç”¨é»˜è®¤é…ç½®...")
            args = {
                "model_name_or_path": "/workspace/llmexp/LLaMA-Factory/Qwen/Qwen2___5-7B-Instruct",
                "adapter_name_or_path": "/workspace/llmexp/saves/qwen2.5-7b_lawyer/lora/sft",
                "template": "Qwen",
                "finetuning_type": "lora",
            }

        self.chat_model = ChatModel(args=args)
        self.chat_history = []
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def extract_law_references(self, text: str) -> List[Tuple[str, str]]:
        """æå–æ–‡æœ¬ä¸­çš„æ³•è§„å¼•ç”¨"""
        law_refs = []
        for pattern in LAW_PATTERNS:
            matches = re.finditer(pattern, text)
            for match in matches:
                law_text = match.group()
                # ç”Ÿæˆæœç´¢å¼•æ“é“¾æ¥ï¼ˆä½¿ç”¨ç™¾åº¦ï¼‰
                search_url = f"https://www.baidu.com/s?wd={law_text}"
                law_refs.append((law_text, search_url))
        return law_refs

    def add_law_links(self, text: str) -> str:
        """ä¸ºæ³•è§„å¼•ç”¨æ·»åŠ è¶…é“¾æ¥"""
        result = text
        # ä½¿ç”¨ HTML æ ‡è®°æ·»åŠ è¶…é“¾æ¥
        for pattern in LAW_PATTERNS:
            def replace_with_link(match):
                law_text = match.group()
                search_url = f"https://www.baidu.com/s?wd={law_text}"
                return f'<a href="{search_url}" target="_blank" style="color: #1E88E5; text-decoration: underline;">{law_text}</a>'
            result = re.sub(pattern, replace_with_link, result)
        return result

    def format_history_for_model(self, history: List[Tuple[str, str]]) -> List[dict]:
        """å°†èŠå¤©å†å²è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼"""
        messages = []
        for user_msg, assistant_msg in history:
            messages.append({"role": "user", "content": user_msg})
            if assistant_msg:
                messages.append({"role": "assistant", "content": assistant_msg})
        return messages

    def chat(self, message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤

        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            history: èŠå¤©å†å²è®°å½•

        Returns:
            Tuple[assistant_message, updated_history]
        """
        try:
            # æ ¼å¼åŒ–å†å²è®°å½•
            formatted_history = self.format_history_for_model(history)
            formatted_history.append({"role": "user", "content": message})

            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
            response = self.chat_model.chat(
                formatted_history,
                max_new_tokens=512,
                temperature=0.8,
                top_p=0.9,
            )

            # æå–å›å¤æ–‡æœ¬ï¼ˆæ³¨æ„ï¼šresponse æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼‰
            response_text = response[0].response_text

            # ä¸ºæ³•è§„å¼•ç”¨æ·»åŠ è¶…é“¾æ¥
            assistant_message_with_links = self.add_law_links(response_text)

            # æ›´æ–°å†å²è®°å½•
            history.append((message, assistant_message_with_links))

            return assistant_message_with_links, history

        except Exception as e:
            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            return error_msg, history

    def stream_chat(self, message: str, history: List[Tuple[str, str]]):
        """
        æµå¼èŠå¤©ç”Ÿæˆ

        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            history: èŠå¤©å†å²è®°å½•

        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # æ ¼å¼åŒ–å†å²è®°å½•
            formatted_history = self.format_history_for_model(history)
            formatted_history.append({"role": "user", "content": message})

            # æµå¼ç”Ÿæˆ
            full_response = ""
            for new_token in self.chat_model.stream_chat(
                formatted_history,
                max_new_tokens=512,
                temperature=0.8,
                top_p=0.9,
            ):
                full_response += new_token
                yield full_response

            # ä¸ºæ³•è§„å¼•ç”¨æ·»åŠ è¶…é“¾æ¥
            full_response_with_links = self.add_law_links(full_response)
            yield full_response_with_links

        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"

    def clear_history(self):
        """æ¸…é™¤èŠå¤©å†å²"""
        return [], []


def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    # åˆå§‹åŒ–åº”ç”¨
    app = LawyerChatApp()

    # è‡ªå®šä¹‰ CSS
    custom_css = """
    .chat-message {
        padding: 12px 16px;
        border-radius: 12px;
        margin: 8px 0;
    }
    .user-message {
        background-color: #E3F2FD;
        margin-left: auto;
        max-width: 80%;
    }
    .assistant-message {
        background-color: #F5F5F5;
        margin-right: auto;
        max-width: 80%;
    }
    a {
        color: #1E88E5;
        text-decoration: underline;
    }
    a:hover {
        color: #0D47A1;
    }
    """

    # åˆ›å»ºç•Œé¢
    with gr.Blocks(
        title="å¾‹å¸ˆ AI åŠ©æ‰‹",
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="gray",
        ),
        css=custom_css,
    ) as interface:
        gr.Markdown(
            """
            # ğŸ›ï¸ å¾‹å¸ˆ AI åŠ©æ‰‹

            åŸºäº LLaMA-Factory å¾®è°ƒçš„ Qwen2.5-7B æ³•å¾‹å¤§æ¨¡å‹

            **åŠŸèƒ½ç‰¹ç‚¹ï¼š**
            - ğŸ“š æ³•å¾‹å’¨è¯¢ä¸æ¡ˆä¾‹åˆ†æ
            - ğŸ” æ³•è§„æ¡æ–‡æ™ºèƒ½æ£€ç´¢
            - ğŸ”— è‡ªåŠ¨ç”Ÿæˆæ³•è§„é“¾æ¥ï¼ˆç‚¹å‡»å³å¯è·³è½¬æœç´¢å¼•æ“æŸ¥è¯¢ï¼‰
            - ğŸ’¬ æµç•…çš„å¯¹è¯ä½“éªŒ
            """
        )

        with gr.Row():
            with gr.Column(scale=4):
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=500,
                    show_copy_button=True,
                    bubble_full_width=False,
                )

                with gr.Row():
                    msg = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ­£å½“é˜²å«ï¼Ÿ",
                        scale=4,
                        show_label=False,
                    )
                    submit = gr.Button("å‘é€", variant="primary", scale=1)

                with gr.Row():
                    clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")

                gr.Examples(
                    examples=[
                        "ä»€ä¹ˆæ˜¯æ­£å½“é˜²å«ï¼Ÿ",
                        "åŠ³åŠ¨åˆåŒè§£é™¤çš„æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "è¯·è§£é‡Šä¸€ä¸‹ä¾µæƒè´£ä»»æ³•çš„åŸºæœ¬åŸåˆ™",
                        "äº¤é€šäº‹æ•…ä¸­çš„è´£ä»»è®¤å®šæœ‰å“ªäº›æ ‡å‡†ï¼Ÿ",
                        "åˆ‘æ³•ä¸­å…³äºæ•…æ„ä¼¤å®³ç½ªçš„æ„æˆè¦ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ",
                    ],
                    inputs=msg,
                    label="ç¤ºä¾‹é—®é¢˜",
                )

            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ å‚æ•°è®¾ç½®")

                temperature = gr.Slider(
                    minimum=0.1,
                    maximum=2.0,
                    value=0.8,
                    step=0.1,
                    label="æ¸©åº¦ (Temperature)",
                    info="å€¼è¶Šä½è¾“å‡ºè¶Šç¡®å®šï¼Œå€¼è¶Šé«˜è¾“å‡ºè¶Šéšæœº"
                )

                max_tokens = gr.Slider(
                    minimum=64,
                    maximum=1024,
                    value=512,
                    step=64,
                    label="æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max Tokens)",
                    info="æ§åˆ¶å›å¤çš„æœ€å¤§é•¿åº¦"
                )

                gr.Markdown("### ğŸ“– ä½¿ç”¨è¯´æ˜")
                gr.Markdown(
                    """
                    1. åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ³•å¾‹é—®é¢˜
                    2. ç‚¹å‡»"å‘é€"æŒ‰é’®æˆ–æŒ‰å›è½¦é”®
                    3. å›å¤ä¸­çš„æ³•è§„æ¡æ–‡ä¼šè‡ªåŠ¨æ·»åŠ è¶…é“¾æ¥
                    4. ç‚¹å‡»é“¾æ¥å¯åœ¨æ–°çª—å£æŸ¥çœ‹æœç´¢ç»“æœ
                    5. å¯ä»¥éšæ—¶æ¸…ç©ºå¯¹è¯å†å²
                    """
                )

                gr.Markdown("### ğŸ”— æœç´¢å¼•æ“")
                gr.Markdown(
                    """
                    æ³•è§„é“¾æ¥ä½¿ç”¨**ç™¾åº¦æœç´¢**ï¼Œç‚¹å‡»æ³•è§„åç§°æˆ–æ³•æ¡å·å³å¯æŸ¥è¯¢è¯¦ç»†å†…å®¹ã€‚
                    """
                )

        # äº‹ä»¶ç»‘å®š
        submit.click(
            fn=app.chat,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot],
        ).then(
            fn=lambda: "",
            outputs=msg,
        )

        msg.submit(
            fn=app.chat,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot],
        ).then(
            fn=lambda: "",
            outputs=msg,
        )

        clear_btn.click(
            fn=app.clear_history,
            outputs=[chatbot],
        )

    return interface


if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()

    # å¯åŠ¨ Gradio åº”ç”¨
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        quiet=False,
    )
